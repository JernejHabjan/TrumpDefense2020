https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ


Article

doi:10.1038/nature24270

Mastering the game of Go without
human knowledge

David Silver1*, Julian Schrittwieser1*, Karen Simonyan1*, Ioannis Antonoglou1, Aja Huang1, Arthur Guez1,
Thomas Hubert1, Lucas Baker1, Matthew Lai1, Adrian Bolton1, Yutian Chen1, Timothy Lillicrap1, Fan Hui1, Laurent Sifre1,
George van den Driessche1, Thore Graepel1 & Demis Hassabis1


Our new method uses a deep neural network fθ with parameters θ.
This neural network takes as an input the raw board representation s
of the position and its history, and outputs both move probabilities and
a value, (p, v) =​  fθ(s). The vector of move probabilities p represents the
probability of selecting each move a (including pass), pa =​  Pr(a|​s). The
value v is a scalar evaluation, estimating the probability of the current
player winning from position s. This neural network combines the roles
of both policy network and value network12 into a single architecture.
The neural network consists of many residual blocks4 of convolutional
layers16,17 with batch normalization18 and rectifier nonlinearities19 (see
Methods).
The neural network in AlphaGo Zero is trained from games of selfplay by a novel reinforcement learning algorithm. In each position s,
an MCTS search is executed, guided by the neural network fθ. The
MCTS search outputs probabilities π of playing each move. These
search probabilities usually select much stronger moves than the raw
move probabilities p of the neural network fθ(s); MCTS may therefore
be viewed as a powerful policy improvement operator20,21. Self-play
with search—using the improved MCTS-based policy to select each
move, then using the game winner z as a sample of the value—may
be viewed as a powerful policy evaluation operator. The main idea of
our reinforcement learning algorithm is to use these search operators
repeatedly in a policy iteration procedure22,23: the neural network’s
parameters are updated to make the move probabilities and value (p,
v) =​  fθ(s) more closely match the improved search probabilities and selfplay winner (π, z); these new parameters are used in the next iteration
of self-play to make the search even stronger. Figure 1 illustrates the
self-play training pipeline.
The MCTS uses the neural network fθ to guide its simulations (see
Fig. 2). Each edge (s, a) in the search tree stores a prior probability
P(s, a), a visit count N(s, a), and an action value Q(s, a). Each simulation
starts from the root state and iteratively selects moves that maximize
an upper confidence bound Q(s, a) +​  U(s, a), where U(s, a) ∝​  P(s, a) / 
(1 +​  N(s, a)) (refs 12, 24), until a leaf node s′ is encountered. This leaf
position is expanded and evaluated only once by the network to gene­
rate both prior probabilities and evaluation, (P(s′​, ·),V(s′​))  =​  fθ(s′​).
Each edge (s, a) traversed in the simulation is updated to increment its
visit count N(s, a), and to update its action value to the mean evaluation
over these simulations, Q(s, a ) = 1/N (s, a ) ∑ s ′|s , a → s ′ V (s ′) where
s, a→​s′ indicates that a simulation eventually reached s′​after taking
move a from position s.
MCTS may be viewed as a self-play algorithm that, given neural
network parameters θ and a root position s, computes a vector of search
probabilities recommending moves to play, π =​  αθ(s), proportional to
the exponentiated visit count for each move, πa ∝​  N(s, a)1/τ, where τ is
a temperature parameter.
The neural network is trained by a self-play reinforcement learning
algorithm that uses MCTS to play each move. First, the neural network
is initialized to random weights θ0. At each subsequent iteration i ≥​  1,
games of self-play are generated (Fig. 1a). At each time-step t, an MCTS
search πt = α θi −1(s t ) is executed using the previous iteration of neural
network fθi −1 and a move is played by sampling the search probabilities
πt. A game terminates at step T when both players pass, when the
search value drops below a resignation threshold or when the game
exceeds a maximum length; the game is then scored to give a final
reward of rT ∈​  {−​1,+​1} (see Methods for details). The data for each
time-step t is stored as (st, πt, zt), where zt =​  ±​rT is the game winner
from the perspective of the current player at step t. In parallel (Fig. 1b),
new network parameters θi are trained from data (s, π, z) sampled
uniformly among all time-steps of the last iteration(s) of self-play. The
neural network (p, v ) = fθi (s ) is adjusted to minimize the error between
the predicted value v and the self-play winner z, and to maximize the
similarity of the neural network move probabilities p to the search
probabilities π. Specifically, the parameters θ are adjusted by gradient
descent on a loss function l that sums over the mean-squared error and
cross-entropy losses, respectively:

(p, v ) = fθ (s ) and l = (z − v )2 − πT logp + c θ

where c is a parameter controlling the level of L2 weight regularization
(to prevent overfitting).










Neural network architecture. The input to the neural network is a 19 ×​  19  ×​  17
image stack comprising 17 binary feature planes. Eight feature planes, Xt, consist
of binary values indicating the presence of the current player’s stones ( X it =1 if
intersection i contains a stone of the player’s colour at time-step t; 0 if the intersec­
tion is empty, contains an opponent stone, or if t <​ 0). A further 8 feature planes,
Yt, represent the corresponding features for the opponent’s stones. The final feature
plane, C, represents the colour to play, and has a constant value of either 1 if black
is to play or 0 if white is to play. These planes are concatenated together to give
input features st =​  [Xt, Yt, Xt−1, Yt−1,..., Xt−7, Yt−7, C]. History features Xt, Yt are
necessary, because Go is not fully observable solely from the current stones, as
repetitions are forbidden; similarly, the colour feature C is necessary, because the
komi is not observable.
The input features st are processed by a residual tower that consists of a single
convolutional block followed by either 19 or 39 residual blocks4.
The convolutional block applies the following modules:
(1) A convolution of 256 filters of kernel size 3 ×​ 3 with stride 1
(2) Batch normalization18
(3) A rectifier nonlinearity
Each residual block applies the following modules sequentially to its input:
(1) A convolution of 256 filters of kernel size 3 ×​ 3 with stride 1
(2) Batch normalization
(3) A rectifier nonlinearity
(4) A convolution of 256 filters of kernel size 3 ×​ 3 with stride 1
(5) Batch normalization
(6) A skip connection that adds the input to the block
(7) A rectifier nonlinearity
The output of the residual tower is passed into two separate ‘heads’ for
computing the policy and value. The policy head applies the following modules:
(1) A convolution of 2 filters of kernel size 1 ×​ 1 with stride 1
(2) Batch normalization
(3) A rectifier nonlinearity
(4) A fully connected linear layer that outputs a vector of size 192 +​  1  =​  362,
corresponding to logit probabilities for all intersections and the pass move
The value head applies the following modules:
(1) A convolution of 1 filter of kernel size 1 ×​ 1 with stride 1
(2) Batch normalization
(3) A rectifier nonlinearity
(4) A fully connected linear layer to a hidden layer of size 256
(5) A rectifier nonlinearity
(6) A fully connected linear layer to a scalar
(7) A tanh nonlinearity outputting a scalar in the range [−​1, 1]
The overall network depth, in the 20- or 40-block network, is 39 or 79 parameterized layers,
respectively, for the residual tower, plus an additional 2 layers for
the policy head and 3 layers for the value head.
We note that a different variant of residual networks was simultaneously applied
to computer Go33 and achieved an amateur dan-level performance; however, this
was restricted to a single-headed policy network trained solely by supervised
learning.

