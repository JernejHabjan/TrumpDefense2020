Deep Learning
Ian Goodfellow
Yoshua Bengio
Aaron Courville

Introduction:
	Hardcoding rules using logical inference rules, known as Knowledge base approach to artificial intelligence.
	AI retrieving knowledge by extracting patterns in raw data, known as machine learning.
	Feature is a piece of information that is selected as atribute.
	Representation learning is approach, that discovers representation to output and representation itself - example are Shallow autoencoders
	Deep learning solves central problem in representation learning by introducing representations that are rexpressed in terms of other, simpler representations.
	It enables computer to build complex concepts out of simpler concepts (corners and edges to determine slope)
	Example of deep learning model is Multilayer perceptron (MLP), which is a mathematical function mapping inputs to outputs.
	Two ways to measure depth of model:
		the depth of thecomputational graph - Number of sequential instructions that must be executed to evaluate the architecture - longest path through flow chart
		the depth of the probabilistic modeling graph - Depth of grapth describing how concepts are related to each other - depth of the flowchart of the computations needed to compute the representation of each concepts may be much deeper than graph of concepts themselves.
	Venn diagram of deep learning -> https://i.imgur.com/Jezptsv.png
	Flowchart showing different parts of AI. Shaded boxes indicate components that are able tolearn from data -> https://i.imgur.com/lY0pGv1.png
	Book structure -> https://i.imgur.com/KVbNYfl.png
PART I - Applied Math and Machine Learning Basics
	Chapter 2 - Linear Algebra
		2.1 Scalars, Vectors, Matrices and Tensors
			Tensors:
				It is an array of numbers arranged on a regular grid with a variable number of axes
		2.7 Eigendecomposition
			Is decomposition of matrix which decomposites matrix into eigenvectors and eigenvalues
				Aneigenvectorof a square matrixAis a nonzero vectorvsuch that multi-plication by A alters only the scale of v: Av = λv.
		2.8 Singular Value Decomposition
			decomposing matrix into singular vectors and singular values
	Chapter 3 - Probability and InformationTheory
	Chapter 4 - Numerical Computation
		Overflow and underflow
			to stabilize agains overflow and underflow ins softmax function - predict the probabilities with a multinoulli distribution
	Chapter 5 - Machine Learning Basics
		Linear regression - if relation between inputs and outputs is close to linear
		No free lunch theorem
			It says that there is no best machine learning algorithm and in particular no best form of regularization
			We must choose regularization that is well suited to the particular task we want to solve.
		5.2.2 Regularization
			expressing preferences for different solutions, bot implicitly and explicitly
		5.3 Hyperparameters and Validation Sets
			Controlls algorithm behavior
			Validation set is a set that we vertify our model on for overfitting.
		5.7 Supervised learning
			5.7.2 Support Vector Machines
				Unlike logisticregression, the support vector machine does not provide probabilities, but onlyoutputs a class identity.
		5.8 Unsupervised Learning Algorithms
Part II - Deep Networks: ModernPractices
	Modern deep learning provides a powerful framework for supervised learning.By adding more layers and more units within a layer, a deep network can representfunctions of increasing complexity.
	Chapter 6 - Deep Feedforward Networks
		called also Multilayer perceptrons (MLPs)
		Goal is to approximate some function f*.
		There are no feedback connections unlike recurrent neural networks.
		design decisions ofr feedfowrard network:  choosing the optimizer, the cost function, and the form of the output units.
		activation function - used to compute the hidden layer values
		The rectiﬁed linear activation function - recommended activation function for use with most feedfowrard nn
		In modern neural networks,the default recommendation is to use therectiﬁed linear unit, or ReLU
		6.2 Gradient-Based Learning
			6.2.1 Cost Functions
				we use the cross-entropy between the training data and the model’s predictions as the cost function
			6.2.2 Output Unit
				we simply use the cross-entropy between the data distribution and the model distribution.
		6.3 Hidden Units
			Rectiﬁed linear units are an excellent default choice of hidden unit
			most hidden units can be described as accepting a vector of inputs x, computing an aﬃne transformation z=transp(W)x+b, and then applying an element-wise nonlinear function g(z)
			Rectiﬁed linear units use the activation function g(z) = max{0, z}.
		6.4 Architecture Design
			Deeper networks are often able to use far fewer units per layer and far fewer parameters, as well as frequently generalizing to the test set, but they also tend to be harder to optimize.
		6.4.1 Universal Approximation Properties and Depth
			In summary, a feedforward network with a single layer is suﬃcient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly. 
			In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error.
		6.5 Back-Propagation and Other Diﬀerentiation Algorithms
	Chapter 7 - Regularization for Deep Learning
		make an algorithm that will perform well not just on the training data, but also on new inputs
		regularizations - Many strategiesused in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error
		7.1 Parameter Norm Penalties
		7.1.1 L2 regularization (or weight decay) penalizes model parameters for deviating from the ﬁxed value of zero
		7.1.2 L1 Regularization
			Ω(θ) = ||w||1=i|wi|,
			sum of absolute values of the individual parameters
		7.4 Dataset Augmentation
			The best way to make a machine learning model generalize better is to train it on more data
		7.5 Noise Robustness
			add noise to data or to weights
				The Bayesian treatment of learning would consider the model weights to be uncertain and representable via a probability distribution that reﬂects this uncertainty.
		7.8 Early Stopping
			so that validation set error doesnt start rising
		7.9 Parameter Sharing
			Def: regularize parameters to be close to one another, the more popular way is to use constraints: to force sets of parameters to be equal. 
			7.9.1 Convolutional Neural Networks
				CNNs take this property into account by sharing parameters across multiple image locations.
				This means that we can ﬁnd a cat with the same cat detector whether the cat appears at column i or column i + 1 in the image.
		7.11 Bagging and Other Ensemble Methods
			Bagging(short forbootstrap aggregating) is a technique for reducing general-ization error by combining several models
			model averaging
			Bagging involves training multiple modelsand evaluating multiple models on each test example
			This seems impracticalwhen each model is a large neural network, since training and evaluating suchnetworks is costly in terms of runtime and memory. 
	Chapter 8 - Optimization for Training DeepModels		
		8.1 How Learning Diﬀers from Pure Optimization
			The goal of a machine learning algorithm is to reduce the expected generalization error. This quantity is known as the risk.
			The training process based on minimizing this average training error is known as empirical risk minimization
		8.1.3 Batch and Minibatch Algorithms
			Larger batches provide a more accurate estimate of the gradient, but with less than linear returns.
			Multicore architectures are usually underutilized by extremely small batches
			Use batches of power 2 for GPUs
			It is also crucial that the minibatches be selected randomly.
		8.3 Basic Algorithms
			8.3.1 Stochastic Gradient Descent
				A crucial parameter for the SGD algorithm is the learning rate.	
				This is because the SGD gradient estimator introduces a source of noise (therandom sampling ofmtraining examples) that does not vanish even when we arriveat a minimum.
			8.3.2 Momentum
				Momentum aims primarily to solve two problems: poor conditioning of the Hessian matrix and variance in the stochastic gradient
		8.5 Algorithms with Adaptive Learning Rates
			8.5.3 Adam
				adam - adaptive moments
	Chapter 9 - Convolutional Networks
		CNNs are a specialized kind of neural network for processing datathat has a known grid-like topology
		Convolutionalnetworks are simply neural networks that use convolution in place of general matrixmultiplication in at least one of their layers
		The output of convolution is a sequence where each member of the output is a function of a small number of neighboring members of the input
		The idea of parameter sharing manifests in the application of the same convolution kernel at each time step.
		9.1 The Convolution Operation
			you remember this from robotics class
		9.2 Motivation
			Convolutional networks, however, typically have sparse interactions(also referred to as sparse connectivity or sparse weights), as where traditional nnets have each output unit interacts with every input unit
			This is accomplished by making the kernel smaller than the input.
			parameter sharing and equivariant presentations
			Moreover, convolution provides a means for working withinputs of variable size.
		9.3 Pooling
			layer consists of 3 stages:
				1. performs several convolutions in parallel to produce a set of linear activations
				2. detector stage - each linear activation is run through a nonlinear activation function, such as the rectiﬁed linear activation function.
				3. use a pooling function to modify the output of the layer further
				https://i.imgur.com/E09YPKy.png
			A pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs. For example, the max pooling
			In all cases, pooling helps to make the representation approximately invariant to small translations of the input.
		9.4 Convolution and Pooling as an Inﬁnitely Strong Prior
			We can imagine a convolutional net as being similar to a fully connected net,but with an inﬁnitely strong prior over its weights.
		9.5 Variants of the Basic Convolution Function
			Additionally, the input is usually not just a grid of real values. Rather, it is a grid of vector-valued observations.
			One essential feature of any convolutional network implementation is the ability to implicitly zero pad the input V to make it wider.
		9.6 Structured Outputs
			Convolutional networks can be used to output a high-dimensional structured object, rather than just predicting a class label for a classiﬁcation task or a real value for a regression task. 
		9.7 Data Types
			The data used with a convolutional network usually consist of several channels, each channel being the observation of a diﬀerent quantity at some point in space or time.
	Chapter 10 - Sequence Modeling: Recurrent and Recursive Nets
		are a family of neural networks for processing sequential data.
		Rnn needs to have shared parameters across different parts of a model.
		Each member of the output is a function of the previous members of the output
		Each member of the output is produced using the same update rule applied to the previous outputs
		10.1 Unfolding Computational Graphs
		10.2 Recurrent Neural Networks
			"causal" structure meaning that the state at time t captures only information from the past.
			RNN can map an input sequence to a ﬁxed-sizevector.
			RNN can map a ﬁxed-size vector to asequence. 
			RNN canmap an input sequence to an output sequence of the same length
			The computation in most RNNs can be decomposed into three blocks of parametersand associated transformations:
				1. from the input to the hidden state,
				2. from the previous hidden state to the next hidden state, and
				3. from the hidden state to the output.
		10.3 Bidirectional RNNs
			we want to  output a prediction that may depent on the whole input sequence.
			This means we have 2 rnns
			For multi-dimensional input such as images (2d) we have multidimensional rnns (4 rnns)
		10.4 Encoder-Decoder Sequence-to-SequenceArchitectures
			RNN can be trained to map an input sequence to anoutput sequence which is not necessarily of the same length
		10.5 Deep Recurrent Networks
		10.6 Recursive Neural Networks
			is structured as a deep tree, rather than the chain-like structure of RNNs.
	Chapter 11 - Practical Methodology
		11.1 Performance Metrics
			Precision is the fraction of detectionsreported by the model that were correct, while recall is the fraction of true eventsthat were detected.
			When using precision and recall, it is common to plot aPR curve
			F-score
		11.2 Default Baseline Models
			If you want to perform supervised learning with ﬁxed-size vectors as input,use a feedforward network with fully connected layers.
			If the input has knowntopological structure (for example, if the input is an image), use a convolutional network.
				 In these cases, you should begin by using some kind of piecewise linear unit (ReLUs or their generalizations, such as Leaky ReLUs, PreLus, or maxout).
			A reasonable choice of optimization algorithm is SGD with momentum witha decaying learning rate (popular decay schemes that 
				perform better or worseon diﬀerent problems include decaying linearly until reaching a ﬁxed minimumlearning rate, decaying exponentially,
				or decreasing the learning rate by a factor of2–10 each time validation error plateaus).
			Another reasonable alternative is Adam.Batch normalization can have a dramatic eﬀect on optimization performance,especially for convolutional networks and networks with sigmoidal nonlinearities.
			While it is reasonable to omit batch normalization from the very ﬁrst baseline, itshould be introduced quickly if optimization appears to be problematic.
			Unless your training set contains tens of millions of examples or more, youshould include some mild forms of regularization from the start. Early stoppingshould be used almost universally. 
			Dropout is an excellent regularizer that is easyto implement and compatible with many models and training algorithms. 
			Batchnormalization also sometimes reduces generalization error and allows dropout tobe omitted, because of the noise in the estimate of the statistics used to normalizeeach variable.
		11.4.1 Manual Hyperparameter Tuning
			Effect of changing hyperparameters: https://i.imgur.com/5Ep2fHg.png
	Chapter 12 - Applications
		12.1 Large-Scale Deep Learning
			High number of neurons
		12.1.2 GPU Implementations
	
	
	
	
	
	
	
	
	
	
	
	
	